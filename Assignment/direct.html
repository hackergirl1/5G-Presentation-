<!DOCTYPE html>
<html>
<head>
    <link rel = "stylesheet" type = "text/css" href = "home.css">
    <title>Mapping</title>
</head>

<body>
     <header>
        <div id="headimg">
		<img src="mapme.png" alt="main header"width="100%" >
        </div>
        <nav>
        <ul>
            <li class="menuitem"><a href="index.html" ><img alt="" src="home.png" height="15px" width="15px">&nbsp;Home</a></li>
            <li class="menuitem drpdwn"><div id="active"><a href="#"><img alt="" src="note.png" height="15px" width="15px">&nbsp;Types of Mapping</a></div>
                <ul class="drpdwnitems">
                    <li class="drpdwnword"><a href="direct.html">Direct Mapping</a><br></li>
                    <li class="drpdwnword"><a href="associative.html">Associative Mapping</a><br></li>
                    <li class="drpdwnword"><a href="kway.html">K-Way Set Associative Mapping</a><br></li>
                </ul></li>
            <li class="menuitem"><a href="form.html"><img alt="" src="quiz.png" height="15px" width="15px">&nbsp;Quiz</a></li>
        </ul>
    </nav>
    </header>
    <a name="top"></a>
    <div id="maincontent">
        <br><br><br>
        <h1 id=pagehead> Direct Mapping</h1>
		<h2>Definition</h2>
         <p>
			This is the simplest technique, it maps each block of main memory into only one possible cache line. This mapping is expressed as:<br><br>
			<i>i = modulo m</i><br><br>
			 where<br><br>
			 <table>
			 <tr>
			 <td>i</td><td>=</td><td>cache line number</td>
			 </tr>
			 <tr>
			 <td>j</td><td>=</td><td>main memory number</td>
			 </tr>
			 <tr>
			 <td>m</td><td>=</td><td>number of lines in the cache</td>
			 </tr>
			 </table>
		</p>
		<h2>How it works</h2>
		 <p>
			<figure align="center">
			<img src = "dmap.png" alt = ""  height = "350px" width = "578px">
			<figcaption>Figure 1.1: Direct mapping diagram</figcaption>
			</figure><br>
		    Figure 1.1 above shows  the mapping of the first m blocks of main memory. Each block if main memory maps into one unique line of the cache. 
			The next m blocks of main memory map into the cache in the same way; that is, block Bm of main memory maps into line L0 ¬of cache, block 
			Bm+1 maps into line L1, and so on.<br><br>
			When it comes to cache access each main memory address can be viewed as consisting of three fields. The least significant w bits identify a unique 
			word or byte within a block of main memory. The remaining s bits specify one of the 2<sup>s</sup> blocks of main memory. The cache logic interprets these s bits 
			as a tag of s – r bits and a line field of r bits. This latter fiend identifies one of the m = 2<sup>r</sup> lines of the cache.
		</p>
		<h2>Example</h2>
		 <p>
		   <figure align="center">
			<img src = "example1.png" alt = ""  height = "497px" width = "524px">
			<figcaption>Figure 1.2: Direct mapping example diagram</figcaption>
			</figure><br>
			In the example, m = 16K = 214 and i = j modulo 214. The mapping becomes:
			<table>
			 <tr>
			 <th>Cache Line</th><th>Starting Memory Address of Block</th>
			 </tr>
			 <tr>
			 <td>0</td><td>000000, 010000, ..., FF0000</td>
			 </tr>
			 <tr>
			 <td>1</td><td>000004, 010004, ..., FF0004</td>
			 </tr>
			 <tr>
			 <td>.</td><td>.</td>
			 </tr>
			 <tr>
			 <td>.</td><td>.</td>
			 </tr>
			 <tr>
			 <td>.</td><td>.</td>
			 </tr>
			 <tr>
			 <td>2<sup>14</sup>-1</td><td>00FFFC, 01FFFC, ..., FFFFFC</td>
			 </tr>
			 </table><br>
			 Note that no two blocks that map into the same line number have the same tag number.
			 Thus, blocks with starting addresses 000000, 010000, …, FF0000 have tag numbers 00,
			 01, …, FF, respectively.<br><br>
			<figure align="center">
			<img src = "read.png" alt = ""  height = "400px" width = "418px">
			<figcaption>Figure 1.3: Cache read operation</figcaption>
			</figure><br>
			A read operation works as follows. The cache system is presented with a 24-bit address.
			The 14-bit line number is used as an index into the cache to access a particular line. 
			If the 8-bit tag number matches the tag number currently stored in that line, then the 2-bit 
			word number is used to select one of the 4 bytes in that line. Otherwise, the 22-bit tag-plus-line 
			field is used to fetch a block from main memory. The actual address that is used for the fetch is the 
			22-bit tag-plus-line concatenated with two 0 bits, so that 4 bytes are fetched starting on a block boundary.<br><br>
            The effect of this mapping is that blocks of main memory are assigned to lines
            of the cache as follows:
			<figure align="center">
			<img src = "fig1.4.png" alt = ""  height = "103px" width = "285px">
			<figcaption>Figure 1.4: Effect of direct mapping on blocks of main memory</figcaption>
			</figure><br>
			Thus, the use of a portion of the address as a line number provides a unique mapping of each block of main memory into 
			the cache. When a block is actually read into its assigned line, it is necessary to tag the data to distinguish it from 
			other blocks that can fit into that line. The most significant s - r bits serve this purpose. The direct mapping technique
			is simple and inexpensive to implement. Its main disadvantage is that there is a fixed cache location for any given block. 
			Thus, if a program happens to reference words repeatedly from two different blocks that map into the same line, then the blocks 
			will be continually swapped in the cache, and the hit ratio will be low (a phenomenon known as thrashing).
			<br><br>
			<h2>Selective Victim Cache Simulator</h2>
			<br>
			One approach to lower the miss penalty is to remember what was discarded

            in case it is needed again. Since the discarded data has already been fetched, it can
            be used again at a small cost. Such recycling is possible using a victim cache. Victim
            cache was originally proposed as an approach to reduce the conflict misses of direct
            mapped caches without affecting its fast access time. Victim cache is a fully associative
            cache, whose size is typically 4 to 16 cache lines, residing between a direct mapped L1
            cache and the next level of memory.	
            
        </p>
	</div>


</body>
</html>
=======
in case it is needed again. Since the discarded data has already been fetched, it can
be used again at a small cost. Such recycling is possible using a victim cache. Victim
cache was originally proposed as an approach to reduce the conflict misses of direct
mapped caches without affecting its fast access time. Victim cache is a fully associative
cache, whose size is typically 4 to 16 cache lines, residing between a direct mapped L1
cache and the next level of memory.
			
        <h3>References</h3>
	<i>Stallings, William. 2006. Computer Organization and Architecture - Designing for Performance.
     New York: Pearson Education.</i>
			
		 </p>
		<br><a href='#top' class="links">Back to Top</a>
            <br><hr><br>
            Next: <a href="associative.html" class="links">Associative Mapping</a><br>
            <a href="index.html" class="links">Back to home</a>
	</div>
</body>
</html>


